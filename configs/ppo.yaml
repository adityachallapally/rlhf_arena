# PPO Configuration for RLHF Arena
# Proximal Policy Optimization with RLHF

# Model configuration
model:
  checkpoint: "microsoft/DialoGPT-medium"  # Base model checkpoint
  max_length: 512                          # Maximum sequence length
  temperature: 1.0                         # Sampling temperature
  top_p: 0.9                              # Top-p sampling
  top_k: 50                               # Top-k sampling
  repetition_penalty: 1.1                 # Repetition penalty
  do_sample: true                         # Enable sampling
  pad_token_id: null                      # Padding token ID (auto-detect)

# Training configuration
training:
  batch_size: 4                           # Training batch size
  learning_rate: 1e-5                     # Learning rate
  num_epochs: 10                          # Number of training epochs
  gradient_accumulation_steps: 4          # Gradient accumulation steps
  max_grad_norm: 1.0                     # Maximum gradient norm
  warmup_steps: 100                      # Warmup steps
  lr_scheduler: "cosine"                 # Learning rate scheduler
  weight_decay: 0.01                     # Weight decay
  
  # PPO specific parameters
  ppo_epochs: 4                          # PPO epochs per update
  target_kl: 0.01                        # Target KL divergence
  clip_ratio: 0.2                        # PPO clip ratio
  value_clip_ratio: 0.2                  # Value function clip ratio
  gamma: 0.99                            # Discount factor
  gae_lambda: 0.95                       # GAE lambda parameter
  entropy_coef: 0.01                     # Entropy coefficient
  value_coef: 0.5                        # Value function coefficient
  max_kl: 0.01                           # Maximum KL divergence
  
  # Reward configuration
  reward_scale: 1.0                      # Reward scaling factor
  reward_normalization: true              # Enable reward normalization
  reward_clipping: true                   # Enable reward clipping
  reward_clip_range: [-10, 10]           # Reward clipping range

# Dataset configuration
dataset:
  name: "hh"                             # Dataset name (hh, oasst, ultrafeedback)
  split: "train"                         # Dataset split
  max_samples: 10000                     # Maximum samples to use
  validation_split: 0.1                  # Validation split ratio
  preprocessing:
    remove_duplicates: true              # Remove duplicate samples
    filter_by_length: true              # Filter by sequence length
    min_length: 10                      # Minimum sequence length
    max_length: 512                     # Maximum sequence length

# Hardware configuration
hardware:
  device: "auto"                         # Device (auto, cuda, cpu)
  mixed_precision: "fp16"                # Mixed precision (fp16, bf16, none)
  gradient_checkpointing: true           # Enable gradient checkpointing
  ddp_backend: "nccl"                    # Distributed training backend
  num_workers: 4                         # DataLoader workers
  
  # Memory optimization
  max_memory: "24GB"                     # Maximum GPU memory
  memory_efficient_attention: true       # Use memory efficient attention
  flash_attention: false                 # Use flash attention (if available)

# Logging and monitoring
logging:
  output_dir: "experiments/ppo"          # Output directory
  log_level: "INFO"                      # Log level
  save_steps: 100                        # Save checkpoint every N steps
  eval_steps: 200                        # Evaluate every N steps
  logging_steps: 10                      # Log every N steps
  
  # Experiment tracking
  use_wandb: true                        # Use Weights & Biases
  wandb_project: "rlhf_arena"            # W&B project name
  wandb_run_name: "ppo_experiment"       # W&B run name
  use_tensorboard: true                  # Use TensorBoard

# Evaluation configuration
evaluation:
  metrics: ["reward_mean", "kl_divergence", "entropy", "clip_fraction"]
  eval_batch_size: 8                     # Evaluation batch size
  num_eval_samples: 1000                 # Number of evaluation samples
  save_generations: true                 # Save generated samples
  
  # Human evaluation
  human_eval: false                      # Enable human evaluation
  human_eval_samples: 100               # Number of human eval samples

# Checkpointing
checkpointing:
  save_dir: "checkpoints/ppo"            # Checkpoint save directory
  save_best_only: true                   # Save only best model
  save_last: true                        # Save last checkpoint
  max_checkpoints: 5                     # Maximum checkpoints to keep
  
  # Resume training
  resume_from: null                      # Resume from checkpoint
  auto_resume: true                      # Auto-resume from latest

# Hyperparameter search
hyperparameter_search:
  enabled: false                         # Enable hyperparameter search
  method: "optuna"                       # Search method
  n_trials: 20                           # Number of trials
  search_space:
    learning_rate: [1e-6, 1e-4]         # Learning rate range
    batch_size: [2, 8]                  # Batch size range
    clip_ratio: [0.1, 0.3]              # Clip ratio range
